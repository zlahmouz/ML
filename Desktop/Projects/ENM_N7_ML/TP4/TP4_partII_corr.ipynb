{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TP4: Recurrent neural networks"
      ],
      "metadata": {
        "id": "f4LeDfklN7MQ"
      },
      "id": "f4LeDfklN7MQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part II: a generative LSTM"
      ],
      "metadata": {
        "id": "YZSd8CWSN_Ig"
      },
      "id": "YZSd8CWSN_Ig"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the time series in Part I, language processing tasks (generation, translation, automatic correction, etc.) lend themselves to the use of recurrent networks. \\\\\n",
        "But whatever the architecture, perceptron, RNN, or transformers (we will see them later), the neural network approach requires a change in representation: we must move from a sequence of words to a sequence of input vectors.\n",
        "This change in representation involves several steps. The most important steps (tokenization, embedding) are illustrated here through a folk music generation task. \\\n",
        "Yes indeed, music is a language as well!"
      ],
      "metadata": {
        "id": "UooZiEbhOP3A"
      },
      "id": "UooZiEbhOP3A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cb830b-3881-4ab0-a03e-1bfc7a08740e",
      "metadata": {
        "id": "f3cb830b-3881-4ab0-a03e-1bfc7a08740e"
      },
      "outputs": [],
      "source": [
        "# This tutorial is based on MIT pedagogical materials.\n",
        "# First download and import the MIT 6.S191 package:\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "# Import all remaining packages\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "# import functools\n",
        "from tqdm import tqdm\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II.1)** From folk songs to a pytorch loader \\\n",
        "\n",
        "The first exercise consists of building a dataset from many (c.a. 800) music scores.\n",
        "These scores are transcriptions of popular irish songs  in [ABC](https://fr.wikipedia.org/wiki/ABC_(notation)) notation.\n",
        "\n",
        "\n",
        "**Q1** Load the song list and browse some of them. How melodies are encoded ?"
      ],
      "metadata": {
        "id": "fyYfr6SuOrLE"
      },
      "id": "fyYfr6SuOrLE"
    },
    {
      "cell_type": "code",
      "source": [
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "example_song = songs[712]\n",
        "print(\"\\nExample song: \")\n",
        "print(example_song)"
      ],
      "metadata": {
        "id": "v771-MNjOkOY"
      },
      "id": "v771-MNjOkOY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the ABC notation to audio file and listen to it\n",
        "mdl.lab1.play_song(example_song)"
      ],
      "metadata": {
        "id": "FcEZ7WGEQjD3"
      },
      "id": "FcEZ7WGEQjD3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build our dataset, let's first merge all the songs into one text:\n"
      ],
      "metadata": {
        "id": "zcTUss7MRvTD"
      },
      "id": "zcTUss7MRvTD"
    },
    {
      "cell_type": "code",
      "source": [
        "songs_joined = \"\\n\\n\".join(songs)"
      ],
      "metadata": {
        "id": "GjF6DlG2R6fg"
      },
      "id": "GjF6DlG2R6fg",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the problem is to convert a character string into a numerical sequence that can be \"learned\". Typically, this change in representation involves four stages:\n",
        "\n",
        "- [Three preprocessing steps](https://web.archive.org/web/20200131102455/https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/):\n",
        "  * Cleaning: the text is cleaned and formatted in a standard form.\n",
        "  * Tokenization: the text is segmented in elmentary units (eg: letters, words, pieces of words, etc).\n",
        "  * Numericalization: each token is mapped to a numerical id.\n",
        "\n",
        "- An embedding step: the numerical ids are mapped onto tensors. This mapping is usually parameterized by trainable weights. Hence it is done during the learning phase.\n",
        "\n",
        "\n",
        "In this lab, we oversimplify the first three steps: we consider that the musical scores are already normalized and the segmentation is done by character.\n",
        "Moreover, each character is mapped to an integer via the following code:"
      ],
      "metadata": {
        "id": "MDTwhOllmReT"
      },
      "id": "MDTwhOllmReT"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(songs_joined))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "metadata": {
        "id": "S338SQ_JmQ19"
      },
      "id": "S338SQ_JmQ19",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** How big is the “vocabulary” used here? Could we have reduced it? Say how and for what reason."
      ],
      "metadata": {
        "id": "zANxRothSAlN"
      },
      "id": "zANxRothSAlN"
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPnD94nNLpAr",
        "outputId": "9d906ab8-51ad-475e-8c12-e643e7fe50a4"
      },
      "id": "OPnD94nNLpAr",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce the size of *vocab*, it would have been possible to get rid of the non-musical parts of the headers (lines X: to Z:)\n",
        "before the merge, and to use specific characters to represent the words \"Major\" and \"Minor\". Among [other strategies](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/), it would allow to reduce the mean number of bits by character, and ultimately to reach a more compact representation of the text."
      ],
      "metadata": {
        "id": "nCsYHGl8Emsj"
      },
      "id": "nCsYHGl8Emsj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Complete the *vectorize_string* function which converts any subsequence of the text *songs_joined* into np.array of indices."
      ],
      "metadata": {
        "id": "9weuYpBoSX1S"
      },
      "id": "9weuYpBoSX1S"
    },
    {
      "cell_type": "code",
      "source": [
        "### Vectorize the songs string ###\n",
        "\n",
        "'''TODO: Write a function to convert the all songs string to a vectorized\n",
        "    (i.e., numeric) representation. Use the appropriate mapping\n",
        "    above to convert from vocab characters to the corresponding indices.\n",
        "\n",
        "  NOTE: the output of the `vectorize_string` function\n",
        "  should be a np.array with `N` elements, where `N` is\n",
        "  the number of characters in the input string\n",
        "'''\n",
        "def vectorize_string(string):\n",
        "  vectorized_output = np.array([char2idx[char] for char in string])\n",
        "  return vectorized_output\n",
        "\n",
        "\n",
        "vectorized_songs = vectorize_string(songs_joined)"
      ],
      "metadata": {
        "id": "osTDtCcXSc5b"
      },
      "id": "osTDtCcXSc5b",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repr(songs_joined[:10])"
      ],
      "metadata": {
        "id": "WXKv5r14SkrP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "04e2a979-6db9-4b80-8f12-836eb1f4c55d"
      },
      "id": "WXKv5r14SkrP",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'X:1\\\\nT:Alex'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(repr(songs_joined[:10]))"
      ],
      "metadata": {
        "id": "_YqQMH7hSoTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f77032-4df3-4e26-efbe-266522d14e8c"
      },
      "id": "_YqQMH7hSoTJ",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'X:1\\nT:Alex'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataset below is defined in such a way as to be able to generate target sequences shifted one step to the right w.r.t. the input sequences:"
      ],
      "metadata": {
        "id": "YzDXZCHDJ8xs"
      },
      "id": "YzDXZCHDJ8xs"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class BitsOfSongs(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for the time series data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, input_sequence_len):\n",
        "        self.data = data\n",
        "        self.input_sequence_len = input_sequence_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.input_sequence_len - 1\n",
        "\n",
        "    def __getitem__(self, start_idx):\n",
        "        # Extract an input sequence\n",
        "        stop_idx = start_idx + self.input_sequence_len\n",
        "        sequence = self.data[start_idx:stop_idx]\n",
        "        # shift right the extraction window to get the target:\n",
        "        target = self.data[start_idx + 1:stop_idx + 1]\n",
        "        return {'sequence': sequence, 'target': target}"
      ],
      "metadata": {
        "id": "fUWXoTD35tlN"
      },
      "id": "fUWXoTD35tlN",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Instantiate a dataset and a dataloader. Visualize some input and target sequences."
      ],
      "metadata": {
        "id": "bj9mAtmqQ472"
      },
      "id": "bj9mAtmqQ472"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = BitsOfSongs(vectorized_songs, 10)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "I7fr_hlr7-mR"
      },
      "id": "I7fr_hlr7-mR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataloader))\n",
        "x = batch['sequence']\n",
        "y = batch['target']\n",
        "print(x.shape,y.shape)"
      ],
      "metadata": {
        "id": "oEPff5Ae-RaL"
      },
      "id": "oEPff5Ae-RaL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II.2)** Implementation of a LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "mDF5Lx-SSzTK"
      },
      "id": "mDF5Lx-SSzTK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now consider the following model:\n"
      ],
      "metadata": {
        "id": "v3esYv2NTF-o"
      },
      "id": "v3esYv2NTF-o"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class genFolk(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_size=256, hidden_size=50, vocab_size=10,\n",
        "                 batch_size = 32):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, latent_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(latent_size, hidden_size, batch_first=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_cell = (torch.zeros(1, batch_size, hidden_size),\n",
        "                       torch.zeros(1, batch_size, hidden_size))\n",
        "\n",
        "    def forward(self,seq):\n",
        "        seq = self.embedding(seq)\n",
        "        lstm_out, self.hidden_cell = self.lstm(seq, self.hidden_cell)\n",
        "        lstm_out = self.tanh(lstm_out)\n",
        "        pred = self.linear(lstm_out)\n",
        "        return pred\n",
        "\n"
      ],
      "metadata": {
        "id": "9MyrP7StTI4X"
      },
      "id": "9MyrP7StTI4X",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** What role does the *self.embedding* layer play?"
      ],
      "metadata": {
        "id": "6Q_Le8Y8Ri-_"
      },
      "id": "6Q_Le8Y8Ri-_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding step maps token onto vectors  "
      ],
      "metadata": {
        "id": "Kqka4kowL965"
      },
      "id": "Kqka4kowL965"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6** Briefly describe the rest of the model. Compute an output and describe each of its dimensions. Also specify the reason why the batch size is taken as an argument by the class constructor."
      ],
      "metadata": {
        "id": "fK4MVaGSSCNN"
      },
      "id": "fK4MVaGSSCNN"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "input_size = 256\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "out_size = len(vocab)\n",
        "\n",
        "\n",
        "model = genFolk(latent_size=256,\n",
        "                hidden_size=1024,\n",
        "                vocab_size=len(vocab))"
      ],
      "metadata": {
        "id": "KSLCrLbRRqXI"
      },
      "id": "KSLCrLbRRqXI",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = BitsOfSongs(vectorized_songs, 100)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) #, sampler=sampler)\n",
        "batch = next(iter(dataloader))\n",
        "x = batch['sequence']\n",
        "y = batch['target']\n",
        "print(x.shape, y.shape)\n",
        "pred = model(x)\n",
        "print(pred.shape)"
      ],
      "metadata": {
        "id": "xT45k7k_TYCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8128fe-c28a-4f1f-8798-38a40c238cfc"
      },
      "id": "xT45k7k_TYCf",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100]) torch.Size([32, 100])\n",
            "torch.Size([32, 100, 83])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II.2)** Training of the model"
      ],
      "metadata": {
        "id": "p6a6bBuNZiD-"
      },
      "id": "p6a6bBuNZiD-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To measure the difference between the predicted token and the observed token, it is possible to use the same cost function as in classification:"
      ],
      "metadata": {
        "id": "J1Tu19bNTz1_"
      },
      "id": "J1Tu19bNTz1_"
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_loss(y, pred):\n",
        "  trpred = torch.transpose(pred, 1, 2)\n",
        "  return loss_fn(trpred, y)\n",
        "\n",
        "example_batch_loss =  compute_loss(y, pred)\n",
        "\n",
        "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.detach().numpy())"
      ],
      "metadata": {
        "id": "3cgfbNO_T3iA"
      },
      "id": "3cgfbNO_T3iA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6** Why do we need to transpose the prediction tensor to be able to use cross entropy?"
      ],
      "metadata": {
        "id": "bewzWWOqaF5n"
      },
      "id": "bewzWWOqaF5n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross entropy has been coded for computer vision classification/segmentation; in these domains, the second dimension of the output tensors refers to output classes. Here, classes correspond to the third dimension (there is as many classes as characters in *vocab*)."
      ],
      "metadata": {
        "id": "DCIois_3Nn7Z"
      },
      "id": "DCIois_3Nn7Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7** Complete the following training loop, put it on a gpu, and try to get the best loss by tuning the hyperparameters."
      ],
      "metadata": {
        "id": "rBfXQLwjT-P3"
      },
      "id": "rBfXQLwjT-P3"
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyperparameter setting and optimization ###\n",
        "\n",
        "# Optimization parameters:\n",
        "num_epochs = 20  #\n",
        "batch_size = 32  # Experiment between 1 and 64\n",
        "num_samples = 100*batch_size # num of sequences sampled at each epoch\n",
        "seq_length = 100  # Experiment between 50 and 500\n",
        "learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
        "\n",
        "# Model parameters:\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024  # Experiment between 1 and 2048\n",
        "\n",
        "\n",
        "from torch.utils.data import RandomSampler\n",
        "dataset = BitsOfSongs(vectorized_songs, seq_length)\n",
        "sampler = RandomSampler(dataset, replacement=True, num_samples=num_samples)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)"
      ],
      "metadata": {
        "id": "MrCT1WLsT9IA"
      },
      "id": "MrCT1WLsT9IA",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genFolk(latent_size=256,\n",
        "                hidden_size=1024,\n",
        "                vocab_size=len(vocab),\n",
        "                batch_size = batch_size).cuda()\n",
        "\n",
        "\n",
        "optimizer =  torch.optim.Adam(model.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "nmWhXQ30UEMX"
      },
      "id": "nmWhXQ30UEMX",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  for batch in dataloader:\n",
        "\n",
        "    x = batch['sequence'].cuda()\n",
        "    y = batch['target'].cuda()\n",
        "    model.zero_grad()\n",
        "    model.hidden_cell = (torch.zeros(1,batch_size,hidden_size).cuda(),\n",
        "                    torch.zeros(1,batch_size,hidden_size).cuda())\n",
        "\n",
        "    pred = model(x)\n",
        "\n",
        "    loss = compute_loss(y, pred)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # Update the progress bar\n",
        "  history.append(loss.detach().cpu().numpy().mean())\n",
        "  plotter.plot(history)"
      ],
      "metadata": {
        "id": "WVhvlkHuAudR"
      },
      "id": "WVhvlkHuAudR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II.3)** Generation of folk songs"
      ],
      "metadata": {
        "id": "_8CyWJYeUKsX"
      },
      "id": "_8CyWJYeUKsX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate a unique sequence, we will use the trained weights to instantiate a model of the same class with *batch_size*=1:"
      ],
      "metadata": {
        "id": "KeJUBtywUOXU"
      },
      "id": "KeJUBtywUOXU"
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate one sequence\n",
        "batch_size_inference = 1\n",
        "\n",
        "model_bs1 = genFolk(latent_size=256,\n",
        "                hidden_size=1024,\n",
        "                vocab_size=len(vocab),\n",
        "                batch_size = batch_size_inference)\n",
        "\n",
        "model_bs1.load_state_dict(model.state_dict())\n",
        "\n",
        "model_bs1.eval()"
      ],
      "metadata": {
        "id": "t2eg15FLURHR"
      },
      "id": "t2eg15FLURHR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the procedure is as follows:\n",
        "\n",
        "- initialize $h_0$, $c_0$ to 0.\n",
        "- initialize the sequence with the index $i_0$ corresponding to the letter \"X\", since it is with this letter that an ABC code begins.\n",
        "- at each step $n$, use the model to calculate $h_n$, $c_n$ and the output $pred_n$\n",
        "   from $h_{n-1}$, $c_{n-1}$ and $i_{n-1}$.\n",
        "- determine $i_n$ from $pred_n$.\n",
        "\n",
        "This last step is not done by sampling the distribution contained in $pred_n$.\n",
        "\n",
        "These steps are coded below:"
      ],
      "metadata": {
        "id": "TEsWQapSUX7h"
      },
      "id": "TEsWQapSUX7h"
    },
    {
      "cell_type": "code",
      "source": [
        "# nb of steps:\n",
        "generation_length=1000\n",
        "\n",
        "# init hidden & cell states\n",
        "model_bs1.hidden_cell = (torch.zeros(1,1,hidden_size),\n",
        "              torch.zeros(1,1,hidden_size))\n",
        "\n",
        "# Starter:\n",
        "start_string=\"X\"\n",
        "start_ids = [char2idx[s] for s in start_string]\n",
        "start_ids_torch = torch.tensor(start_ids).unsqueeze(dim=0)\n",
        "\n",
        "# init the list of successive i_n\n",
        "text_generated = []\n",
        "\n",
        "# loop for generation:\n",
        "input_eval = start_ids_torch\n",
        "\n",
        "\n",
        "for n in range(generation_length):\n",
        "    predictions = model_bs1(input_eval)\n",
        "\n",
        "    # Remove the batch dimension\n",
        "    predictions = predictions.squeeze(dim=0)\n",
        "\n",
        "    num_sampler = torch.distributions.categorical.Categorical(logits = predictions)\n",
        "    predicted_id = num_sampler.sample()\n",
        "    print(predicted_id)\n",
        "    input_eval = predicted_id.unsqueeze(dim = 0)\n",
        "    text_generated.append(idx2char[predicted_id.numpy()].item())"
      ],
      "metadata": {
        "id": "toUl2I9lUamC"
      },
      "id": "toUl2I9lUamC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8** What step does the call to *torch...Categorical* correspond to? \\\n",
        "Why the *logits = predictions* syntax? \\\n",
        "Why bother with a sampling instead of taking an *argmax* as for classification?\n"
      ],
      "metadata": {
        "id": "dbStkd7KkfJa"
      },
      "id": "dbStkd7KkfJa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Categorical* provides a sampler for the probability laws on *vocab* that are defined in the output. As the softmax has not been applied, the law is represented by its [logits](https://en.wikipedia.org/wiki/Logit)."
      ],
      "metadata": {
        "id": "jAHOLy9MLVrR"
      },
      "id": "jAHOLy9MLVrR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9** Comment on the appearance of the texts generated by the model."
      ],
      "metadata": {
        "id": "LyouenBfqwn3"
      },
      "id": "LyouenBfqwn3"
    },
    {
      "cell_type": "code",
      "source": [
        "text_generated = start_string + ''.join(text_generated)\n",
        "print(text_generated)"
      ],
      "metadata": {
        "id": "f8aZkjNhZ6To"
      },
      "id": "f8aZkjNhZ6To",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The headers generated by the model are likely (see the titles, for example)."
      ],
      "metadata": {
        "id": "7cERTvZ2SU_A"
      },
      "id": "7cERTvZ2SU_A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10** Use the code below to listen to melodies generated by your LSTM:"
      ],
      "metadata": {
        "id": "PBG-kPe9ppXd"
      },
      "id": "PBG-kPe9ppXd"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# To extract a list of potential songs among text_generated:\n",
        "generated_songs = mdl.lab1.extract_song_snippet(text_generated)\n",
        "\n",
        "for i, song in enumerate(generated_songs):\n",
        "  waveform = mdl.lab1.play_song(song)\n",
        "\n",
        "  # if play_song worked, display the audio box:\n",
        "  if waveform:\n",
        "    print(\"Generated song\", i)\n",
        "    ipythondisplay.display(waveform)"
      ],
      "metadata": {
        "id": "U50Y0mFIaCdk"
      },
      "id": "U50Y0mFIaCdk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}